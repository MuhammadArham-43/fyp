{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 23.952095808383234,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.24,
      "grad_norm": 4.385363578796387,
      "learning_rate": 0.0002,
      "loss": 1.5269,
      "step": 10
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.933692216873169,
      "learning_rate": 0.0002,
      "loss": 1.0012,
      "step": 20
    },
    {
      "epoch": 0.72,
      "grad_norm": 2.8795578479766846,
      "learning_rate": 0.0002,
      "loss": 0.8549,
      "step": 30
    },
    {
      "epoch": 0.96,
      "grad_norm": 2.6115949153900146,
      "learning_rate": 0.0002,
      "loss": 0.7169,
      "step": 40
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.1791419982910156,
      "learning_rate": 0.0002,
      "loss": 0.5346,
      "step": 50
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.2472424507141113,
      "learning_rate": 0.0002,
      "loss": 0.5277,
      "step": 60
    },
    {
      "epoch": 1.68,
      "grad_norm": 1.1866775751113892,
      "learning_rate": 0.0002,
      "loss": 0.5178,
      "step": 70
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.9295816421508789,
      "learning_rate": 0.0002,
      "loss": 0.5377,
      "step": 80
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.0023280382156372,
      "learning_rate": 0.0002,
      "loss": 0.4618,
      "step": 90
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.8786582946777344,
      "learning_rate": 0.0002,
      "loss": 0.3609,
      "step": 100
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.9333590865135193,
      "learning_rate": 0.0002,
      "loss": 0.3963,
      "step": 110
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.9337195754051208,
      "learning_rate": 0.0002,
      "loss": 0.3913,
      "step": 120
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.861030638217926,
      "learning_rate": 0.0002,
      "loss": 0.3197,
      "step": 130
    },
    {
      "epoch": 3.35,
      "grad_norm": 1.123089075088501,
      "learning_rate": 0.0002,
      "loss": 0.2827,
      "step": 140
    },
    {
      "epoch": 3.59,
      "grad_norm": 1.2478092908859253,
      "learning_rate": 0.0002,
      "loss": 0.3089,
      "step": 150
    },
    {
      "epoch": 3.83,
      "grad_norm": 1.0045901536941528,
      "learning_rate": 0.0002,
      "loss": 0.3456,
      "step": 160
    },
    {
      "epoch": 4.07,
      "grad_norm": 1.0072121620178223,
      "learning_rate": 0.0002,
      "loss": 0.2837,
      "step": 170
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.985724925994873,
      "learning_rate": 0.0002,
      "loss": 0.2642,
      "step": 180
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.7439349293708801,
      "learning_rate": 0.0002,
      "loss": 0.2739,
      "step": 190
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.968112051486969,
      "learning_rate": 0.0002,
      "loss": 0.2792,
      "step": 200
    },
    {
      "epoch": 5.03,
      "grad_norm": 0.8173796534538269,
      "learning_rate": 0.0002,
      "loss": 0.2685,
      "step": 210
    },
    {
      "epoch": 5.27,
      "grad_norm": 0.8234810829162598,
      "learning_rate": 0.0002,
      "loss": 0.2279,
      "step": 220
    },
    {
      "epoch": 5.51,
      "grad_norm": 0.9549993872642517,
      "learning_rate": 0.0002,
      "loss": 0.2572,
      "step": 230
    },
    {
      "epoch": 5.75,
      "grad_norm": 0.8728552460670471,
      "learning_rate": 0.0002,
      "loss": 0.2508,
      "step": 240
    },
    {
      "epoch": 5.99,
      "grad_norm": 0.7173126339912415,
      "learning_rate": 0.0002,
      "loss": 0.2551,
      "step": 250
    },
    {
      "epoch": 6.23,
      "grad_norm": 0.966556966304779,
      "learning_rate": 0.0002,
      "loss": 0.2199,
      "step": 260
    },
    {
      "epoch": 6.47,
      "grad_norm": 0.8497243523597717,
      "learning_rate": 0.0002,
      "loss": 0.2233,
      "step": 270
    },
    {
      "epoch": 6.71,
      "grad_norm": 0.9622923731803894,
      "learning_rate": 0.0002,
      "loss": 0.2571,
      "step": 280
    },
    {
      "epoch": 6.95,
      "grad_norm": 0.8814608454704285,
      "learning_rate": 0.0002,
      "loss": 0.2392,
      "step": 290
    },
    {
      "epoch": 7.19,
      "grad_norm": 0.5044333934783936,
      "learning_rate": 0.0002,
      "loss": 0.2054,
      "step": 300
    },
    {
      "epoch": 7.43,
      "grad_norm": 0.5802822709083557,
      "learning_rate": 0.0002,
      "loss": 0.2106,
      "step": 310
    },
    {
      "epoch": 7.66,
      "grad_norm": 0.5550259351730347,
      "learning_rate": 0.0002,
      "loss": 0.2215,
      "step": 320
    },
    {
      "epoch": 7.9,
      "grad_norm": 0.7285711765289307,
      "learning_rate": 0.0002,
      "loss": 0.2466,
      "step": 330
    },
    {
      "epoch": 8.14,
      "grad_norm": 0.5466397404670715,
      "learning_rate": 0.0002,
      "loss": 0.2247,
      "step": 340
    },
    {
      "epoch": 8.38,
      "grad_norm": 0.504767894744873,
      "learning_rate": 0.0002,
      "loss": 0.2042,
      "step": 350
    },
    {
      "epoch": 8.62,
      "grad_norm": 0.5297210216522217,
      "learning_rate": 0.0002,
      "loss": 0.2019,
      "step": 360
    },
    {
      "epoch": 8.86,
      "grad_norm": 0.4260992109775543,
      "learning_rate": 0.0002,
      "loss": 0.2174,
      "step": 370
    },
    {
      "epoch": 9.1,
      "grad_norm": 0.458048552274704,
      "learning_rate": 0.0002,
      "loss": 0.2281,
      "step": 380
    },
    {
      "epoch": 9.34,
      "grad_norm": 0.8252569437026978,
      "learning_rate": 0.0002,
      "loss": 0.204,
      "step": 390
    },
    {
      "epoch": 9.58,
      "grad_norm": 0.6463214755058289,
      "learning_rate": 0.0002,
      "loss": 0.2135,
      "step": 400
    },
    {
      "epoch": 9.82,
      "grad_norm": 0.6545379161834717,
      "learning_rate": 0.0002,
      "loss": 0.2083,
      "step": 410
    },
    {
      "epoch": 10.06,
      "grad_norm": 0.39772048592567444,
      "learning_rate": 0.0002,
      "loss": 0.1964,
      "step": 420
    },
    {
      "epoch": 10.3,
      "grad_norm": 0.5492293834686279,
      "learning_rate": 0.0002,
      "loss": 0.1895,
      "step": 430
    },
    {
      "epoch": 10.54,
      "grad_norm": 0.4852510690689087,
      "learning_rate": 0.0002,
      "loss": 0.1927,
      "step": 440
    },
    {
      "epoch": 10.78,
      "grad_norm": 1.0195435285568237,
      "learning_rate": 0.0002,
      "loss": 0.2013,
      "step": 450
    },
    {
      "epoch": 11.02,
      "grad_norm": 0.6625113487243652,
      "learning_rate": 0.0002,
      "loss": 0.2059,
      "step": 460
    },
    {
      "epoch": 11.26,
      "grad_norm": 0.5610945820808411,
      "learning_rate": 0.0002,
      "loss": 0.1838,
      "step": 470
    },
    {
      "epoch": 11.5,
      "grad_norm": 0.42912814021110535,
      "learning_rate": 0.0002,
      "loss": 0.1904,
      "step": 480
    },
    {
      "epoch": 11.74,
      "grad_norm": 0.4531083405017853,
      "learning_rate": 0.0002,
      "loss": 0.1994,
      "step": 490
    },
    {
      "epoch": 11.98,
      "grad_norm": 0.5135875344276428,
      "learning_rate": 0.0002,
      "loss": 0.2087,
      "step": 500
    },
    {
      "epoch": 12.22,
      "grad_norm": 0.6213065981864929,
      "learning_rate": 0.0002,
      "loss": 0.1629,
      "step": 510
    },
    {
      "epoch": 12.46,
      "grad_norm": 0.5326986312866211,
      "learning_rate": 0.0002,
      "loss": 0.1853,
      "step": 520
    },
    {
      "epoch": 12.69,
      "grad_norm": 0.5473726987838745,
      "learning_rate": 0.0002,
      "loss": 0.19,
      "step": 530
    },
    {
      "epoch": 12.93,
      "grad_norm": 0.8465375304222107,
      "learning_rate": 0.0002,
      "loss": 0.1978,
      "step": 540
    },
    {
      "epoch": 13.17,
      "grad_norm": 0.4705387055873871,
      "learning_rate": 0.0002,
      "loss": 0.1808,
      "step": 550
    },
    {
      "epoch": 13.41,
      "grad_norm": 0.34286925196647644,
      "learning_rate": 0.0002,
      "loss": 0.1853,
      "step": 560
    },
    {
      "epoch": 13.65,
      "grad_norm": 0.5431718826293945,
      "learning_rate": 0.0002,
      "loss": 0.1841,
      "step": 570
    },
    {
      "epoch": 13.89,
      "grad_norm": 0.31799548864364624,
      "learning_rate": 0.0002,
      "loss": 0.1929,
      "step": 580
    },
    {
      "epoch": 14.13,
      "grad_norm": 0.3971391022205353,
      "learning_rate": 0.0002,
      "loss": 0.1826,
      "step": 590
    },
    {
      "epoch": 14.37,
      "grad_norm": 0.46682173013687134,
      "learning_rate": 0.0002,
      "loss": 0.1752,
      "step": 600
    },
    {
      "epoch": 14.61,
      "grad_norm": 0.3395557701587677,
      "learning_rate": 0.0002,
      "loss": 0.1891,
      "step": 610
    },
    {
      "epoch": 14.85,
      "grad_norm": 0.45019668340682983,
      "learning_rate": 0.0002,
      "loss": 0.1796,
      "step": 620
    },
    {
      "epoch": 15.09,
      "grad_norm": 0.643670380115509,
      "learning_rate": 0.0002,
      "loss": 0.189,
      "step": 630
    },
    {
      "epoch": 15.33,
      "grad_norm": 0.5187690258026123,
      "learning_rate": 0.0002,
      "loss": 0.1779,
      "step": 640
    },
    {
      "epoch": 15.57,
      "grad_norm": 0.37660884857177734,
      "learning_rate": 0.0002,
      "loss": 0.1783,
      "step": 650
    },
    {
      "epoch": 15.81,
      "grad_norm": 0.34820985794067383,
      "learning_rate": 0.0002,
      "loss": 0.1814,
      "step": 660
    },
    {
      "epoch": 16.05,
      "grad_norm": 0.5118468403816223,
      "learning_rate": 0.0002,
      "loss": 0.1911,
      "step": 670
    },
    {
      "epoch": 16.29,
      "grad_norm": 0.29454293847084045,
      "learning_rate": 0.0002,
      "loss": 0.1605,
      "step": 680
    },
    {
      "epoch": 16.53,
      "grad_norm": 0.3127989172935486,
      "learning_rate": 0.0002,
      "loss": 0.1757,
      "step": 690
    },
    {
      "epoch": 16.77,
      "grad_norm": 0.42663562297821045,
      "learning_rate": 0.0002,
      "loss": 0.1802,
      "step": 700
    },
    {
      "epoch": 17.01,
      "grad_norm": 0.40833303332328796,
      "learning_rate": 0.0002,
      "loss": 0.1931,
      "step": 710
    },
    {
      "epoch": 17.25,
      "grad_norm": 0.49143537878990173,
      "learning_rate": 0.0002,
      "loss": 0.1633,
      "step": 720
    },
    {
      "epoch": 17.49,
      "grad_norm": 0.3581913113594055,
      "learning_rate": 0.0002,
      "loss": 0.1699,
      "step": 730
    },
    {
      "epoch": 17.72,
      "grad_norm": 0.5033524036407471,
      "learning_rate": 0.0002,
      "loss": 0.173,
      "step": 740
    },
    {
      "epoch": 17.96,
      "grad_norm": 0.424411803483963,
      "learning_rate": 0.0002,
      "loss": 0.1907,
      "step": 750
    },
    {
      "epoch": 18.2,
      "grad_norm": 0.5153009295463562,
      "learning_rate": 0.0002,
      "loss": 0.1576,
      "step": 760
    },
    {
      "epoch": 18.44,
      "grad_norm": 0.4068981409072876,
      "learning_rate": 0.0002,
      "loss": 0.1693,
      "step": 770
    },
    {
      "epoch": 18.68,
      "grad_norm": 0.294294148683548,
      "learning_rate": 0.0002,
      "loss": 0.1738,
      "step": 780
    },
    {
      "epoch": 18.92,
      "grad_norm": 0.2685222923755646,
      "learning_rate": 0.0002,
      "loss": 0.174,
      "step": 790
    },
    {
      "epoch": 19.16,
      "grad_norm": 0.2971532344818115,
      "learning_rate": 0.0002,
      "loss": 0.166,
      "step": 800
    },
    {
      "epoch": 19.4,
      "grad_norm": 0.329366534948349,
      "learning_rate": 0.0002,
      "loss": 0.1671,
      "step": 810
    },
    {
      "epoch": 19.64,
      "grad_norm": 0.23961152136325836,
      "learning_rate": 0.0002,
      "loss": 0.1692,
      "step": 820
    },
    {
      "epoch": 19.88,
      "grad_norm": 0.33924058079719543,
      "learning_rate": 0.0002,
      "loss": 0.1782,
      "step": 830
    },
    {
      "epoch": 20.12,
      "grad_norm": 0.28391554951667786,
      "learning_rate": 0.0002,
      "loss": 0.1668,
      "step": 840
    },
    {
      "epoch": 20.36,
      "grad_norm": 0.2714836597442627,
      "learning_rate": 0.0002,
      "loss": 0.1594,
      "step": 850
    },
    {
      "epoch": 20.6,
      "grad_norm": 0.3508140742778778,
      "learning_rate": 0.0002,
      "loss": 0.1719,
      "step": 860
    },
    {
      "epoch": 20.84,
      "grad_norm": 0.23959597945213318,
      "learning_rate": 0.0002,
      "loss": 0.1787,
      "step": 870
    },
    {
      "epoch": 21.08,
      "grad_norm": 0.22639897465705872,
      "learning_rate": 0.0002,
      "loss": 0.1712,
      "step": 880
    },
    {
      "epoch": 21.32,
      "grad_norm": 0.5899794101715088,
      "learning_rate": 0.0002,
      "loss": 0.1597,
      "step": 890
    },
    {
      "epoch": 21.56,
      "grad_norm": 0.362650603055954,
      "learning_rate": 0.0002,
      "loss": 0.1692,
      "step": 900
    },
    {
      "epoch": 21.8,
      "grad_norm": 0.281277060508728,
      "learning_rate": 0.0002,
      "loss": 0.1723,
      "step": 910
    },
    {
      "epoch": 22.04,
      "grad_norm": 0.24177099764347076,
      "learning_rate": 0.0002,
      "loss": 0.1797,
      "step": 920
    },
    {
      "epoch": 22.28,
      "grad_norm": 0.27988365292549133,
      "learning_rate": 0.0002,
      "loss": 0.1647,
      "step": 930
    },
    {
      "epoch": 22.51,
      "grad_norm": 0.3322067856788635,
      "learning_rate": 0.0002,
      "loss": 0.162,
      "step": 940
    },
    {
      "epoch": 22.75,
      "grad_norm": 0.27216431498527527,
      "learning_rate": 0.0002,
      "loss": 0.1707,
      "step": 950
    },
    {
      "epoch": 22.99,
      "grad_norm": 0.21119798719882965,
      "learning_rate": 0.0002,
      "loss": 0.1703,
      "step": 960
    },
    {
      "epoch": 23.23,
      "grad_norm": 0.5575541257858276,
      "learning_rate": 0.0002,
      "loss": 0.165,
      "step": 970
    },
    {
      "epoch": 23.47,
      "grad_norm": 0.32353442907333374,
      "learning_rate": 0.0002,
      "loss": 0.1589,
      "step": 980
    },
    {
      "epoch": 23.71,
      "grad_norm": 1.2027002573013306,
      "learning_rate": 0.0002,
      "loss": 0.1623,
      "step": 990
    },
    {
      "epoch": 23.95,
      "grad_norm": 0.5474757552146912,
      "learning_rate": 0.0002,
      "loss": 0.1726,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 25,
  "save_steps": 500,
  "total_flos": 5874945556316160.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
